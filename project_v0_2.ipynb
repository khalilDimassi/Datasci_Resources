{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalilDimassi/Datasci_Resources/blob/master/project_v0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 networkx nltk gensim"
      ],
      "metadata": {
        "id": "1SB2waJil51Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026a7a87-520c-45c3-e4ce-825e150df242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "import networkx as nx\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "import nltk.corpus\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "eGyyZllMfdFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "  with open(file_path, 'rb') as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "    text = ''\n",
        "    for page in reader.pages:\n",
        "      text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub('[^a-zA-Z ]', '', text)  # remove punctuation\n",
        "  text = ' '.join(text.split())  # remove extra spaces\n",
        "  return text\n",
        "\n",
        "def standardize_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "def stem_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  words = text.split()\n",
        "  stemmed_words = [stemmer.stem(word) for word in words]\n",
        "  return stemmed_words\n",
        "\n",
        "def remove_stop_words(stemmed_words, custom_stop_words=None):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  if custom_stop_words is not None:\n",
        "    stop_words.update(custom_stop_words)\n",
        "  filtered_words = [word for word in stemmed_words if word not in stop_words]\n",
        "  return filtered_words\n",
        "\n",
        "\n",
        "def lda_segmentation(documents):\n",
        "  dictionary = Dictionary(documents)\n",
        "  corpus = [dictionary.doc2bow(text) for text in documents]\n",
        "  model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "  topic_distributions = model.get_document_topics(corpus)\n",
        "  dominant_topics = [max(topic_distribution, key=topic_distribution.get) for topic_distribution in topic_distributions]\n",
        "\n",
        "  # Segment the text at the boundaries between topic changes\n",
        "  segments = []\n",
        "  current_topic = dominant_topics[0]\n",
        "  current_segment = []\n",
        "  for i in range(len(documents)):\n",
        "    if dominant_topics[i] != current_topic:\n",
        "      segments.append(current_segment)\n",
        "      current_segment = []\n",
        "    current_segment.append(documents[i])\n",
        "  segments.append(current_segment)\n",
        "\n",
        "  return segments\n",
        "\n",
        "\n",
        "def create_graph_from_segments(segments):\n",
        "  graph = nx.Graph()\n",
        "  # Add a node for each segment\n",
        "  for segment in segments:\n",
        "    node_id = len(graph)\n",
        "    graph.add_node(node_id, features=segment)\n",
        "  # Add edges between the nodes based on their similarity\n",
        "  for node_id1 in range(len(graph)):\n",
        "    for node_id2 in range(node_id1 + 1, len(graph)):\n",
        "      similarity = nx.jaccard_coefficient(graph[node_id1]['features'], graph[node_id2]['features'])\n",
        "      if similarity > 0.5:\n",
        "        graph.add_edge(node_id1, node_id2, weight=similarity)\n",
        "  return graph\n"
      ],
      "metadata": {
        "id": "zgTjTCVOl7nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/PMBOK ch11.pdf\")\n",
        "text = clean_text(text)\n",
        "text = standardize_text(text)\n",
        "stems = stem_text(text)\n",
        "words = remove_stop_words(stems, custom_stop_words=None)"
      ],
      "metadata": {
        "id": "zCZKmCOCed49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "ListData = []\n",
        "\n",
        "# Define the regular expression pattern\n",
        "pattern = r'11\\.\\d+(\\.\\d+)* [A-Z][^\\n]*'\n",
        "\n",
        "# Find the first match\n",
        "matches = re.finditer(pattern, text)\n",
        "\n",
        "for match in matches:\n",
        "    # Get the matched title\n",
        "    title = match.group(0)\n",
        "    print(\"Title:\", title.strip())\n",
        "\n",
        "    # Find the start and end positions of the matched title\n",
        "    start = match.start()\n",
        "    end = match.end()\n",
        "\n",
        "    # Find the content following the matched title\n",
        "    content = text[end:]\n",
        "\n",
        "    # Find the next match in the remaining content\n",
        "    next_match = re.search(pattern, content)\n",
        "\n",
        "    if next_match:\n",
        "        # If a next match is found, extract it\n",
        "        next_title = next_match.group(0)\n",
        "        print(\"Next Title:\", next_title.strip())\n",
        "    else:\n",
        "        print(\"No more titles found.\")\n",
        "\n",
        "    print(\"Content:\", content.strip())\n",
        "    print(\"----\")\n",
        "    couple = (title.strip() , content.strip())\n",
        "    ListData.append(couple)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(ListData, columns=['title', 'Body'])\n",
        "df"
      ],
      "metadata": {
        "id": "5Sm9K7HFnocP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}