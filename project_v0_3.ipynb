{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cZAEeH3f1bvJrsBEvO-gVEh-H22zpNWX",
      "authorship_tag": "ABX9TyOeOx3iKi8bgYM8aqBq3pDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalilDimassi/Datasci_Resources/blob/master/project_v0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ozzab6VEPEU1"
      },
      "outputs": [],
      "source": [
        "!pip install -q networkx nltk gensim PyMuPDF transformers huggingface_hub spark-nlp pyspark sentence-transformers levenshtein rake_nltk keybert rebel ultralytics==8.0.20 PyMuPDF roboflow --quiet pdf2image opencv-python\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import re\n",
        "import rebel\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from pdf2image import convert_from_path\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "autocorr_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "autocorr_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "summ_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "summ_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "from keybert import KeyBERT\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "from transformers import pipeline\n",
        "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "rebel_model = AutoModelForSeq2SeqLM.from_pretrained('Babelscape/rebel-large')\n",
        "rebel_tokenizer = AutoTokenizer.from_pretrained('Babelscape/rebel-large')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import SentenceDetectorDLModel\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9Hp0Cy5QJrU",
        "outputId": "a87c8fd1-089f-4478-db0f-3ea4859af7dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path, start=1, finish=None):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF document.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "        start (int): The page number to start extraction (default is 1).\n",
        "        finish (int, optional): The page number to finish extraction (default is None, which extracts until the last page).\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the specified pages.\n",
        "    \"\"\"\n",
        "    with fitz.open(file_path) as doc:\n",
        "        text = ''\n",
        "        for i in range(start-1, finish or len(doc)):\n",
        "            page = doc.load_page(i)\n",
        "            text += page.get_text()\n",
        "        return text"
      ],
      "metadata": {
        "id": "aMgMgVXdQMQh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_content(text, pattern):\n",
        "  \"\"\"\n",
        "  Extract content based on a specified pattern from a text.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text to extract content from.\n",
        "      pattern (str): Regular expression pattern to match content.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: A DataFrame containing extracted subjects and their associated content.\n",
        "  \"\"\"\n",
        "  ListData = []\n",
        "  matches = re.finditer(pattern, text)\n",
        "  for match in matches:\n",
        "    title = match.group(0).strip()\n",
        "    start = match.start()\n",
        "    end = match.end()\n",
        "    content_start = end\n",
        "    next_match = re.search(pattern, text[end:])\n",
        "    if next_match:\n",
        "      content_end = end + next_match.start()\n",
        "    else:\n",
        "      content_end = len(text)\n",
        "    content = text[content_start:content_end].strip()\n",
        "    couple = (title, content)\n",
        "    ListData.append(couple)\n",
        "  df = pd.DataFrame(ListData, columns=['subject', 'content'])\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "bdTvB1XNR16V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text by removing non-ASCII characters, whitespace, punctuation, and digits.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\n397\\n', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+|\\n|\\t|[^\\w\\s]|\\d', '', text)\n",
        "    text = re.sub(r'uu', '', text)\n",
        "    text = re.sub(r'Not For Distribution Sale or Reproduction', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "TPSKktpQQO6q"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_text(text):\n",
        "    \"\"\"\n",
        "    Standardize text by converting it to lowercase.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be standardized.\n",
        "\n",
        "    Returns:\n",
        "        str: Standardized text.\n",
        "    \"\"\"\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "QR_THiU9QQoR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    \"\"\"\n",
        "    Lemmatize text by reducing words to their base form.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be lemmatized.\n",
        "\n",
        "    Returns:\n",
        "        str: Lemmatized text.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)"
      ],
      "metadata": {
        "id": "WOiLiF4wQSgu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text, custom_stop_words=None):\n",
        "    \"\"\"\n",
        "    Remove stop words from a list of words.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text from which to remove stop words.\n",
        "        custom_stop_words (list, optional): Custom stop words to include in addition to the default English stop words.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with stop words removed.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if custom_stop_words is not None:\n",
        "        stop_words.update(custom_stop_words)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    filtered_text = \" \".join(filtered_words)\n",
        "    return filtered_text\n",
        "\n"
      ],
      "metadata": {
        "id": "M5R6zjGHQUB-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = clean_text(text)\n",
        "    text = standardize_text(text)\n",
        "    text = standardize_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    text = remove_stop_words(text)\n",
        "    return text\n",
        "\n",
        "def clean_sentences_array(sentences_array):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences_array:\n",
        "        cleaned_sentences.append(clean(sentence))\n",
        "    return cleaned_sentences\n",
        "\n",
        "def clean_dataframe(dataframe, columns):\n",
        "    for column in columns:\n",
        "        if column == 'sentences_array':\n",
        "            dataframe[column] = dataframe[column].apply(clean_sentences_array)\n",
        "        else:\n",
        "            dataframe[column] = dataframe[column].apply(clean)"
      ],
      "metadata": {
        "id": "TFlZwPi4k0r8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_token_length(subject, content, def_tokenizer):\n",
        "    inputs = def_tokenizer.encode_plus(subject + ': ' + content, return_tensors='pt')\n",
        "    return len(inputs['input_ids'][0])"
      ],
      "metadata": {
        "id": "BwRLXSvKpMoE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, tokenizer, model):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "id": "jLHMXzWxpe1l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text, kw_model):\n",
        "    keywords = kw_model.extract_keywords(text)\n",
        "    return [keyword[0] for keyword in keywords]"
      ],
      "metadata": {
        "id": "uTnamRqv7DD4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in nltk.corpus.wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return list(set(synonyms))  # Remove duplicates"
      ],
      "metadata": {
        "id": "z7BVuZ08K8zy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triplets(sentence, tokenizer, model):\n",
        "    inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    triplets = tokenizer.decode(outputs[0])\n",
        "    return triplets"
      ],
      "metadata": {
        "id": "NzDsEFbJhOjx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PDF_REMOVE_TXT(pdfTXT , pdfNoTXT):\n",
        "    # Open the PDF document\n",
        "    doc = fitz.open(pdfTXT)\n",
        "    for page in doc:\n",
        "        # Create a redaction annotation that covers the entire page\n",
        "        rect = fitz.Rect(0, 0, page.rect.width, page.rect.height)\n",
        "        page.add_redact_annot(rect)\n",
        "\n",
        "    # Apply redactions to remove all text\n",
        "    for page in doc:\n",
        "        page.apply_redactions()\n",
        "\n",
        "    # Save the modified PDF without text\n",
        "    doc.save(pdfNoTXT)\n",
        "\n",
        "    # Close the document\n",
        "    doc.close()"
      ],
      "metadata": {
        "id": "B9KrJPD7tiI9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PDF_TO_IMG(frompath , topath):\n",
        "  # Convert the PDF to a list of images (one image per page)\n",
        "  images = convert_from_path(frompath)\n",
        "  image_paths=[]\n",
        "  # Iterate through the images\n",
        "  for i, image in enumerate(images):\n",
        "      # Save each image as a file (optional)\n",
        "      i = i+1\n",
        "      ii = topath + \"/page_\"+str(i)+\".png\"\n",
        "      image_paths.append(ii)\n",
        "      image.save(f'{topath}/page_{i}.png', 'PNG')\n",
        "  return image_paths"
      ],
      "metadata": {
        "id": "6c2yNij9tjx9"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/outputcutimage\n",
        "def cutimage(pathimage , x, y, width, height , i) :\n",
        "\n",
        "  # Load the image\n",
        "  image = cv2.imread(pathimage)\n",
        "\n",
        "  # Crop the image using the bounding box coordinates\n",
        "  cropped_image = image[y:height, x:width]\n",
        "  img = \"/content/outputcutimage/figure\"+str(i)+\".png\"\n",
        "  cv2.imwrite(img, cropped_image, [cv2.IMWRITE_PNG_COMPRESSION, 0])"
      ],
      "metadata": {
        "id": "wx1b6DE9uYSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modeldetect(imagepath , i):\n",
        "  modelyolo = YOLO(\"/content/drive/MyDrive/Colab Notebooks/modelfigbest.pt\")\n",
        "  image = Image.open(imagepath)\n",
        "  image = np.asarray(image)\n",
        "  results = modelyolo.predict(image)\n",
        "  labels = [\"figure\"]\n",
        "  if(len(results[0].boxes.boxes)>0):\n",
        "    #  print(\"************************\" + str(len(results[0].boxes.boxes)) + str(i))\n",
        "     if(imagepath == imgpath[54]):\n",
        "        for box in results[0].boxes.boxes.numpy():\n",
        "            # Extract the box coordinates\n",
        "            x_min, y_min, x_max, y_max, confidence, class_id = box\n",
        "            # print(type(x_min))\n",
        "            # print(x_min, y_min, x_max, y_max, confidence, class_id)\n",
        "            cutimage(imagepath , int(x_min) , int(y_min) , int(x_max) , int(y_max) , i)\n",
        "            i = i + 1\n",
        "     else :\n",
        "      # Extract the box coordinates\n",
        "      if(len(results[0].boxes.boxes) > 1):\n",
        "            x_min, y_min, x_max, y_max, confidence, class_id = results[0].boxes.boxes.numpy()[1]\n",
        "      else :\n",
        "            x_min, y_min, x_max, y_max, confidence, class_id = results[0].boxes.boxes.numpy()[0]\n",
        "      # print(type(x_min))\n",
        "      # print(x_min, y_min, x_max, y_max, confidence, class_id)\n",
        "      if(confidence > 0,9):\n",
        "            cutimage(imagepath , int(x_min) , int(y_min) , int(x_max) , int(y_max) , i)\n",
        "            i = i + 1\n",
        "  return i"
      ],
      "metadata": {
        "id": "i_rBE5NvyVLP"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pathimage(text,path):\n",
        "    # Define the regular expression pattern\n",
        "    pattern = r'11\\.\\d+ [A-Z][^\\n]*'\n",
        "    # Define a regular expression pattern to match a sentence.\n",
        "    sentence_pattern = r\"(Figure|Table) \\d+-\\d+\\.\\s[\\w\\s, &():]+\\n\"\n",
        "    listPath=[]\n",
        "\n",
        "    # Find all matches\n",
        "    matches = re.finditer(pattern, text)\n",
        "\n",
        "    for match in matches:\n",
        "        Chapterpath = []\n",
        "        # Get the matched title\n",
        "        title = match.group(0).strip()\n",
        "\n",
        "        # Find the start and end positions of the matched title\n",
        "        start = match.start()\n",
        "        end = match.end()\n",
        "\n",
        "        # Find the content following the matched title\n",
        "        content_start = end\n",
        "        next_match = re.search(pattern, text[end:])\n",
        "\n",
        "        if next_match:\n",
        "            # If a next match is found, extract its start position\n",
        "            content_end = end + next_match.start()\n",
        "        else:\n",
        "            # If no next match is found, use the end of the text\n",
        "            content_end = len(text)\n",
        "\n",
        "        # Extract the content (paragraph) for this title\n",
        "        content = text[content_start:content_end].strip()\n",
        "        # Use re.search() to find the first sentence in the paragraph.\n",
        "        mtt = re.finditer(sentence_pattern, content)\n",
        "\n",
        "        for m in mtt:\n",
        "          extracted_sentence = m.group(0)\n",
        "          numerofig = int(extracted_sentence.split(\"\\n\")[0].split(\".\")[0].split(\"-\")[1])\n",
        "          if(numerofig == 1) :\n",
        "            numerofig = 5\n",
        "\n",
        "          elif(numerofig > 4 ):\n",
        "            numerofig = numerofig + 1\n",
        "          pathimg = path+\"/figure\"+str(numerofig)+\".png\"\n",
        "          Chapterpath.append(pathimg)\n",
        "        listPath.append(Chapterpath)\n",
        "    return listPath"
      ],
      "metadata": {
        "id": "zf-Y5KKs0HKn"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MPBOK ch11:\n",
        "pdf_file_path = '/content/drive/MyDrive/Colab Notebooks/PMBOK ch11.pdf'\n",
        "extraction_start_page = 3\n",
        "extraction_finish_page = None  # None means extract until the last page\n",
        "pattern_to_extract = r'11\\.\\d+\\.\\d+.\\d+ [A-Z][^\\n]*'\n",
        "\n",
        "text = extract_text_from_pdf(pdf_file_path, extraction_start_page, extraction_finish_page)\n",
        "df = extract_content(text, pattern_to_extract)\n",
        "\n",
        "# load df into sparkdf\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "documentAssembler = DocumentAssembler().setInputCol(\"content\").setOutputCol(\"document\")\n",
        "sentenceDetectorDL = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"en\").setInputCols([\"document\"]).setOutputCol(\"sentences\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    documentAssembler,\n",
        "    sentenceDetectorDL\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzbuFEWSQZEW",
        "outputId": "43177ee5-d738-4b36-d108-6e2b265a9196"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(spark_df)\n",
        "result = model.transform(spark_df)\n",
        "result_df = result.toPandas()\n",
        "result_df['sentences_array'] = result_df['sentences'].apply(lambda x: [i['result'] for i in x])"
      ],
      "metadata": {
        "id": "bJYfdmWAg9ae"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workDF = result_df.copy()"
      ],
      "metadata": {
        "id": "nSXpcltujtos"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workDF.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A0VSKaxnAEG",
        "outputId": "33388495-8802-481f-c009-5c75765b0939"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82 entries, 0 to 81\n",
            "Data columns (total 5 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   subject          82 non-null     object\n",
            " 1   content          82 non-null     object\n",
            " 2   document         82 non-null     object\n",
            " 3   sentences        82 non-null     object\n",
            " 4   sentences_array  82 non-null     object\n",
            "dtypes: object(5)\n",
            "memory usage: 3.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dataframe(workDF, ['subject', 'content', 'sentences_array'])"
      ],
      "metadata": {
        "id": "ECmJO6q0g4J0"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicate & object extraction\n",
        "workDF['subject_tr'], workDF['predicate'], workDF['object'] = zip(*workDF['sentences_array'].apply(lambda x: [extract_triplets(sentence, rebel_tokenizer, rebel_model) for sentence in x]))"
      ],
      "metadata": {
        "id": "3mKYjZPnrlC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords extraction\n",
        "workDF['keywords'] = workDF['content'].apply(lambda x: extract_keywords(x, kw_model))"
      ],
      "metadata": {
        "id": "JLHrrbMY6DV3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Synonyms extraction\n",
        "workDF['synonyms'] = workDF['keywords'].apply(lambda x: [get_synonyms(word) for word in x])"
      ],
      "metadata": {
        "id": "yVKgHxYb6az4"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply summary on content\n",
        "workDF['definitions'] = workDF['content'].apply(lambda x: summarize_text(x, summ_tokenizer, summ_model))"
      ],
      "metadata": {
        "id": "RPka2Kc1pfHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove specific tags\n",
        "workDF['definitions'] = workDF['definitions'].apply(lambda x: re.sub(r'<pad>|</s>', '', x))"
      ],
      "metadata": {
        "id": "M9pgonu4pgcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF to images\n",
        "!mkdir /content/imagesrc\n",
        "frompath = \"/content/drive/MyDrive/Colab Notebooks/PMBOK ch11.pdf\"\n",
        "topath = \"/content/imagesrc\"\n",
        "imgpath = PDF_TO_IMG(frompath , topath)"
      ],
      "metadata": {
        "id": "YYQpePfetAGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for path in imgpath :\n",
        "   i = modeldetect(path , i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ShMVrku7N4",
        "outputId": "9f2d4376-b0dd-4193-c0d1-e453cf3cd51b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "WARNING ⚠️ NMS time limit 0.550s exceeded\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pptext = extract_text_from_pdf(pdf_file_path, extraction_start_page, extraction_finish_page)\n",
        "listepathimage = pathimage(pptext,\"/content/outputcutimage\")\n",
        "print(listepathimage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDtgfC-2yXbE",
        "outputId": "4088679b-91f0-4050-8562-95d001fb5cd2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['/content/outputcutimage/figure2.png', '/content/outputcutimage/figure3.png', '/content/outputcutimage/figure4.png', '/content/outputcutimage/figure5.png', '/content/outputcutimage/figure6.png'], ['/content/outputcutimage/figure7.png', '/content/outputcutimage/figure8.png'], ['/content/outputcutimage/figure9.png', '/content/outputcutimage/figure10.png', '/content/outputcutimage/figure11.png'], ['/content/outputcutimage/figure12.png', '/content/outputcutimage/figure13.png', '/content/outputcutimage/figure15.png', '/content/outputcutimage/figure16.png'], ['/content/outputcutimage/figure17.png', '/content/outputcutimage/figure17.png', '/content/outputcutimage/figure18.png'], ['/content/outputcutimage/figure19.png', '/content/outputcutimage/figure20.png'], ['/content/outputcutimage/figure21.png', '/content/outputcutimage/figure22.png']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workDF"
      ],
      "metadata": {
        "id": "08i6F-tI0Jrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}